# LoCoBench Configuration File
# Configuration for 4 Java projects: 2 hard + 2 expert scenarios

# =============================================================================
# GENERAL CONFIGURATION - API & System Settings
# =============================================================================
retrieval:
  chunk_size: 512
  difficulties:
    - hard
    - expert
  enabled: true
  top_percent: 0.05
  max_context_tokens: 2048
  method: embedding
  model_name: all-MiniLM-L6-v2
  local_model_path: null
  top_k: 5

api:
  # API Keys (set via environment variables for security)
  openai_api_key: "
  openai_base_url: 
  custom_model_name: "DeepSeekR1-70B-LRI"
  openai_timeout: 600
  custom_model_base_url:
  custom_model_api_key:
  custom_model_temperature: 0.1
  custom_model_max_tokens: 1000000
  custom_model_timeout: 600
  custom_model_client_timeout: 3600
  disable_proxy: false
  
  # Rate limiting settings
  max_requests_per_minute: 1
  max_concurrent_requests: 1
  
  # Default models
  default_model_openai: "gpt-oss-120b"
  default_model_google: "gemini-2.5-pro"

data:
  # Local storage directories
  output_dir: "./data/output"
  generated_dir: "./data/generated"

# =============================================================================
# PHASE 1 CONFIGURATION - Project Specification Generation
# =============================================================================

phase1:
  # Supported programming languages - Java only
  supported_languages:
    - java
  
  # Project generation distribution - 4 Java projects total
  projects_per_language: 4
  
  # Complexity levels distribution for synthetic projects
  complexity_distribution:
    hard: 0.5     # 50% hard projects (2 projects)
    expert: 0.5   # 50% expert projects (2 projects)

# =============================================================================
# PHASE 2 CONFIGURATION - Synthetic Codebase Generation  
# =============================================================================

phase2:
  # File count constraints per project
  min_files_per_project: 10
  max_files_per_project: 100
  
  # Generation quality controls (ENFORCED with retry logic)
  min_complexity_score: 0.3
  max_complexity_score: 1.0
  min_documentation_ratio: 0.01

# =============================================================================
# PHASE 3 CONFIGURATION - Long-Context Evaluation Scenario Creation
# =============================================================================

phase3:
  # Scale parameters - 4 scenarios total (1 per project)
  total_instances: 4
  
  # Task category distribution (must sum to total_instances)
  # Selected 4 categories, 1 scenario per category
  task_distribution:
    architectural_understanding: 1    # Deep architectural analysis
    cross_file_refactoring: 1        # Multi-file code restructuring
    feature_implementation: 1        # Complex feature development
    bug_investigation: 1             # Real-world debugging scenarios
  
  # Difficulty distribution (must sum to total_instances)
  difficulty_distribution:
    easy: 0      # 10K-100K tokens 
    medium: 0    # 100K-200K tokens
    hard: 2      # 200K-500K tokens (2 scenarios)
    expert: 2    # 500K-1000K tokens (2 scenarios)
  
  # Context length ranges (min_tokens, max_tokens)
  context_ranges:
    easy: [200000, 500000]
    medium: [200000, 500000]
    hard: [200000, 500000]
    expert: [500000, 1000000]
  
  # Information coverage requirements (ENFORCED with retry logic)
  min_information_coverage: 0.20
  
  # Coverage ranges that determine difficulty levels (RANGE-BASED)
  coverage_ranges:
    easy: [0.20, 0.40]
    medium: [0.40, 0.60]
    hard: [0.60, 0.80]
    expert: [0.80, 1.00]

# =============================================================================
# PHASE 4 CONFIGURATION - Automated Validation & Evaluation
# =============================================================================

phase4:
  # Metric weights for LCBS (LoCoBench Score) - 4 Evaluation Dimensions
  metric_weights:
    software_engineering: 0.40       # Software Engineering Excellence (40% - 8 metrics)
    functional_correctness: 0.30     # Functional Correctness (30% - 4 metrics)
    code_quality: 0.20              # Code Quality Assessment (20% - 3 metrics)  
    longcontext_utilization: 0.10   # Long-Context Utilization (10% - 2 metrics)
  
  # Software Engineering Excellence metric weights (within the 40% software_engineering category)
  software_engineering_weights:
    architectural_coherence: 0.125
    dependency_traversal: 0.125
    cross_file_reasoning: 0.125
    system_thinking: 0.125
    robustness: 0.125
    comprehensiveness: 0.125
    innovation: 0.125
    solution_elegance: 0.125
    
  # Long-Context Utilization metric weights (within the 10% longcontext_utilization category)  
  longcontext_utilization_weights:
    information_coverage: 0.50
    multi_session_memory: 0.50
  
  # Scoring thresholds (ENFORCED for pass/fail determination)
  score_thresholds:
    excellent:
      min: 4.0
      max: 5.0
    good:
      min: 3.0
      max: 4.0
    fair:
      min: 2.0
      max: 3.0
    poor:
      min: 0.0
      max: 2.0
  
  # Timeout settings (seconds) - ENFORCED with asyncio.wait_for()
  task_timeout: 1800      # 30 minutes per task
  session_timeout: 3600   # 60 minutes per session
